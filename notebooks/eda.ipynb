{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final place to do the ML modeling including encoding/scaling/writing model\n",
    "# Later on we can separate the encoding and scaling part to separate file but that will require storing scaled datasets separately\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Read the train and test set\n",
    "X = pd.read_csv('../../data/processed/X.csv').values\n",
    "y = pd.read_csv('../../data/processed/y.csv').values\n",
    "\n",
    "# If y is 1D we will have to flatten the array\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to convert the dataset to a mixed format including scaling/normalization/labelencoding/onehotencoding\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(random_state = 0)\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.675 0.675 0.975 0.95  1.    0.875 0.8   0.775 0.775 0.725]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save results of the cross validation output\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = cross_val_score(estimator=classifier,X=X, y=y,cv=10, scoring='accuracy')\n",
    "print(cv)\n",
    "\n",
    "# Saving the encoder\n",
    "joblib.dump(sc, '../../models/encoders/X_scalar.pkl')\n",
    "\n",
    "# Saving the model\n",
    "classifier_model = open('../../models/classifier.pkl', 'wb')\n",
    "joblib.dump(classifier, classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [[25, 190000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sc.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
